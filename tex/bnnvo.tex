\documentclass[11pt]{amsart}
\usepackage{geometry}                % See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   % ... or a4paper or a5paper or ... 
%\geometry{landscape}                % Activate for for rotated page geometry
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{epstopdf}
\DeclareGraphicsRule{.tif}{png}{.png}{`convert #1 `dirname #1`/`basename #1 .tif`.png}

\input{preamble}


\title{Notes}
\author{Elliot Creager}
%\date{}                                           % Activate to display a given date or no date

\begin{document}
\maketitle
\section{Bayesian neural networks: a variational optimization perspective}
\subsection{Problem statement}
$(x, t)$ are labeled tuples that you can sample from the training data $\mathcal{D}_{train}$.
$\mathcal{D}_{test}$ is the test data distribution, which you \emph{actually} care about performing well on.
Consider training a neural network according to the optimization problem
\begin{align}\label{eq:nnopt}
\underset{w}{\minimize} \quad \mathcal{L}_{train}(w)
\end{align}
with 
\begin{align}
\mathcal{L}_{train}(w) \triangleq \underset{x, y \sim \mathcal{D}_{train}}{\E} \ell (f(x, w), y) + ||w||^2_2
\end{align}

The trouble is that neural networks are \emph{too} good at optimizing (\ref{eq:nnopt}).
I.e., because $\mathcal{L}_{train}$ and $\mathcal{L}_{test}$ are quite different functions of $w$, then $w^*_{train}$ might not perform so well with respect to $\mathcal{L}_{test}$.

Instead we can use variational optimization, which says that 
\begin{align}
\min \mathcal{L}_{train}(w) \leq 
\underbrace{
\underset{w \sim q}{\E} \left[ \mathcal{L}_{train}(w) \right]
}_{\triangleq U_{train}(w)}
\end{align}
for arbitrary choice of $q$. 
Optimizing $U_{train}(w)$ requires that we get good performance in expectation across reasonably likely values of $w$.
\subsection{Gaussian variational optimization}
Say $w \sim \Norm(\mu, \sigma^2)$.
Then we have 
\begin{align}\label{eq:voopt}
\underset{w}{\minimize} \quad U_{train}(\mu, \sigma)
\end{align}
\begin{align}\label{eq:var-bound}
U_{train}(\mu, \sigma) = \underset{w \sim \Norm(\mu, \sigma)}{\E} \left[ \underset{x, y \sim \mathcal{D}_{train}}{\E} \left[ \ell(f(x, w), y) \right] \right].
\end{align}
We want to solve (\ref{eq:voopt}) by gradient based optimization, but we can't differentiate through the $w \sim \Norm(\mu, \sigma)$ operation. 
Fortunately, we can apply the reparameterization trick to rewrite (\ref{eq:var-bound}) as 
\begin{align}\label{eq:var-bound-reparam}
U_{train}(\mu, \sigma) = \underset{\epsilon \sim \Norm(0, I)}{\E} \left[ \underset{x, y \sim \mathcal{D}_{train}}{\E} \left[ \ell(f(x, \sigma\epsilon+\mu), y) \right] \right],
\end{align}
which is differentiable w.r.t. $\mu$ and $\sigma$.

I omitted the weight decay terms, but these should be added in later.

\end{document}  