<!doctype html>
<meta charset="utf-8">
<meta name="viewport" content="width=1080">

<!-- <script src="https://distill.pub/template.v1.js"></script> -->
<script src="assets/lib/template.v1.js"></script>
<script src="assets/lib/d3.v4.min.js"></script>
<script src="assets/lib/d3-contour.v1.min.js"></script>
<script src="assets/lib/d3-scale-chromatic.v1.min.js"></script>
<script src="assets/lib/katex.min.js"></script>
<script src="assets/lib/auto-render.min.js"></script>
<script src="assets/lib/pseudocode.min.js"></script>
<link rel="stylesheet" type="text/css" href="assets/lib/katex.min.css">
<link rel="stylesheet" type="text/css" href="styles.css">
<link rel="stylesheet" type="text/css" href="assets/widgets.css">
<link rel="stylesheet" type="text/css" href="assets/lib/pseudocode.min.css">




<script type="text/front-matter">
    title: "Bayesian Neural Networks"
    description: "By combining neural networks with Bayesian inference, we can learn a probability distribution over possible models. With a simple modification to standard neural network tools, we can mitigate overfitting, learn from small datasets, and express uncertainty about our predictions."
    authors:
    - Jerry Qinghui Yu: http://www.cs.toronto.edu/~jerry/
    - Elliot Creager: http://www.cs.toronto.edu/~creager/
    - David Duvenaud: http://www.cs.toronto.edu/~duvenaud/
    affiliations:
    - University of Toronto
    - University of Toronto, Vector Institute
    - University of Toronto, Vector Institute
</script>

<title>Bayesian Neural Networks</title>

<dt-article id="article" class="centered">
    <link rel="stylesheet" type="text/css" href="assets/widgets.css">
    <script src="assets/utils.js"></script>
    <!-- <script src="assets/lib/lib.js"></script> -->
    <figure style="width:100%; height:400px;">
            <div id="full_bnn" class='.l-screen' style="width:100%; height:400px;"></div>
    </figure>

    <H1>Bayesian Neural Networks</H1>
    <p>
    Bayesian inference allows us to learn a probability distribution over possible neural networks.
    We can approximately solve inference with a simple modification to standard neural network tools.
    The resulting algorithm mitigates overfitting, enables learning from small datasets, and tells us how uncertain our predictions are.
    </p>

    <figure>
        <div id="bnn_graph" style="position:relative; width:650px; height:200px;"></div>
        <button type="button" onclick="full_bnn.train()">Train</button>
        <button type="button" onclick="full_bnn.reset()">Reset</button>
    </figure>

    <dt-byline></dt-byline>

    <h2>What's wrong with neural networks?</h2>
    <p>
        You may have heard deep neural networks described as <i>powerful function approximators</i>.
        Their power is due to the extreme flexibility of having many model parameters (the weights and biases) whose values can be learned from data via gradient-based optimization.
        Because they are good at approximating functions (input-output relationships) when lots of data are available, neural networks are well-suited to artificial intelligence tasks like speech recognition and image classification.
    </p>

    <p>
        But the extreme flexibility of neural networks has a downside: they are particularly vulnerable to <i>overfitting</i>.
        Overfitting happens when the learning algorithm does such a good job of tuning the model parameters for performance on the training set&mdash;by optimizing its <i>objective function</i>&mdash;that the performance on new examples suffers.
        Deep neural networks have a ton of parameters (typically millions in modern models), which essentially guarantees eventual overfitting because the learning algorithm can always do just a little bit better on the training set by tweaking some of the many knobs available to it.
        The flexibility of neural networks during training time actually makes them brittle at test time.
        This might sound surprising at first, but let's look at the training procedure both mathematically and graphically (for a toy problem) to build some intuition around why deep neural networks overfit.
    </p>

    <!-- Regular MLP objective surfaces -->
    <figure style="position:relative; width:984px;">
        <div class="container">
            <div style="display: table; height: 100%">
                <div class='table-cell-wrap'>
                    <figcaption id="nn_loss_train_title" class='little_caption'>
                        Training likelihood $\mathcal{L}_{\text{train}}(w_1, w_2)$
                    </figcaption>
                    <div id="nn_loss_train" class='contour'> </div>
                </div>
                <div class='table-cell-wrap'>
                    <figcaption id="NNLosscaption" style="position: relative; width: 250px; left: 30px; right: 30px; top: 10px;">
                        The neural network objective $\mathcal{L}$ is a function of both the weights and the dataset.
                        Here we show $\mathcal{L}_{\text{train}}$ and $\mathcal{L}_{\text{test}}$ as a function of only two of the neuron weights, with the remaining weights fixed to pre-trained values.
                        Bluer is better.
                        Use your mouse to change the values of the weights for these neurons, or press 'Train' to automatically learn their values using gradient descent.
                        Notice how weight values affect both the training and test objectives (left, right) and the network predictions changes (below).
                    </figcaption>
                </div>
                <div class="table-cell-wrap">
                    <figcaption id="nn_loss_test_title" class='little_caption'>
                        Test likelihood $\mathcal{L}_{\text{train}}(w_1, w_2)$
                    </figcaption>
                    <div id="nn_loss_valid" class='contour'> </div>
                </div>
            </div>
        </div>

        <div class="container" style="height: 250px; top: 40px;">
            <div style="display: table; height: 100%">
                <div class='table-cell-wrap'>
                    <figcaption id="nn_full_title" class='little_caption'>
                        Prediction function $\hat y_w(x)$. Red dots are training data, green are test.
                    </figcaption>
                    <div id="nn_full" class='bordered' style="width:440px; height: 200px;"></div>
                </div>
                <div class='table-cell-wrap'>
                    <figcaption id="nn_graph_title" class='little_caption'>
                        Neural net architecture with the edge between neurons colored according to their shared weight value. In this demo all weights except the first two are fixed.
                        <!-- Notice how the choice of architecture affects the contours of the $\mathcal{L}_{\text{train}}$ and $\mathcal{L}_{\text{test}}$, and that simpler architectures make the two functions more similar. -->
                    </figcaption>
                    <div id="nn_graph" style="width:440px; height: 200px;"></div>
                </div>
                <div class='table-cell-wrap' style="width: 100px;">
                    <form onclick="mlp.update()" style="top: 50px;">
                        <input type="radio" name="net_type" value="Deep" checked> Deep<br>
                        <input type="radio" name="net_type" value="Shallow"> Shallow<br>
                        <input type="radio" name="net_type" value="Linear"> Linear<br>
                    </form>
                    <button type="button" onclick="mlp.train()">Train</button>
                    <button type="button" onclick="mlp.reset()">Reset</button>
                </div>
            </div>
        </div>
    </figure>



    <p>
        First, some notation: let's call inputs $x$ and outputs $y$ and the training set $\mathcal{D}_{\text{train}}$, which is a finite collection of input-output pairs.
        Similarly we have a test set $\mathcal{D}_{\text{test}}$ of <i>different</i> input-output pairs.
        Then neural network training corresponds to the following optimization problem:
    </p>
    <p>
        $\underset{w}{\text{maximize}} \quad \mathcal{L}_{\text{train}}(w)$
    </p>
    <p>
        where $\mathcal{L}_{\text{train}}$, the <i>expected log likelihood</i> of the training data, is a function of the weights $w$, and is defined as
    </p>
    <p>
        $\mathcal{L}_{\text{train}}(w) \coloneqq \mathbb{E}_{x, y \sim \mathcal{D}_{\text{train}}} \left[ \log p(\hat y(x) = y|w) \right]$
        .
    </p>
    <p>
        Here $\log p(\hat y(x) = y|w)$ represents the log likelihood<dt-fn>
            Adding weight decay to this objective is easy, we simply add a prior on $w$ to the log likelihood, expressed as $\log p(w) = \lambda ||w||_2^2$ (up to an additive constant).
            </dt-fn>
        of a particular output $y$ being predicted given the input $x$, which we want to be high on average across all of the training examples<dt-fn>
        We could equivalently write down the optimization problem as minimizing an expected loss instead of maximizing an expected log likelihood.
        In classification, using cross entropy loss is equivalent to modeling output predictions according to a Categorical (i.e., multinomial) distribution
    </dt-fn>.
        Since we'll use $p(\hat y(x) = y|w)$ often, let's name it the <i>single network likelihood</i>; we might write $p(\hat y|w)$ for short, when the context is clear.
    </p>
    <p>
        During training we optimize $\mathcal{L}_{\text{train}}(w)$ with respect to $w$, but we <i>actually</i> care about performing well on the test data, as measured by $\mathcal{L}_{\text{test}}(w)$.
        In the demo above, we plot these functions side-by-side as a function of two weights<dt-fn>
        In this example, we've pre-trained all the weights in the neural network except for two weights in the first layer, which we call $w_1$ and $w_2$.
    </dt-fn>.
        Notice that the optimal weights $w^* = \underset{w}{\text{arg max}}\mathcal{L}_{\text{train}}(w) $ yield a suboptimal test loss $\mathcal{L}_{\text{test}}(w^*)$.
        This indicates overfitting, which manifests as the neural net generalizing poorly: it makes predictions $\hat y(x)$ that hug closely to the training data at the expense of deviating from the test data.

        Scaling up to networks with millions of weights amplifies the severity of overfitting, since $\mathcal{L}_{\text{train}}(w)$ and $\mathcal{L}_{\text{test}}(w)$ tend to be much more dissimilar in high dimensions.
        Indeed, one key insight from deep learning is that the more capacity<dt-fn>
        Very often "capacity" is shorthand for "depth"
    </dt-fn>
        a model has, the better it is at finding local optima on the training loss, thus the more it will eventually overfit.
        Our demo also illustrates that overfitting is less of an issue for low-capacity models: if we restrict the network to have only one hidden layer ("shallow") or no activations ("linear") we see that $\mathcal{L}_{\text{train}}$ looks similar to $\mathcal{L}_{\text{test}}$, so there's little danger of overfitting.
    </p>

    <h2>Bayes to the rescue</h2>
    <p>
        The problem of overfitting is certainly not unique to neural networks.
        But because the flexibility of neural networks makes them particularly susceptible, researchers and practitioners have come up with many extensions to the standard learning algorithm (early stopping, weight decay, and dropout, just to name a few) to reduce overfitting<dt-cite key="hinton2012improving"></dt-cite>.
    </p>

    <p>
        This tutorial focuses on <i>Bayesian inference</i>, a powerful framework that not only helps with overfitting, but also tells us how uncertain our model is about its parameters.
        Before we used the training data $\mathcal{D}_{\text{train}}$ plus a gradient-based optimizer to tune the weights $w$ in order to maximize $\mathcal{L}_{\text{train}}(w)$.
        In Bayesian inference, instead of learning the parameter values, we seek to compute $p(w | \mathcal{D}_{\text{train}})$, the conditional distribution of the weights given the training data.
        $p(w|\mathcal{D}_{\text{train}})$ is called the <i>posterior distribution</i>, or often the <i>posterior</i> for short.
    </p>

    <h3>Exact Bayesian inference</h3>
    <p>
        Exact Bayesian inference tells us how to compute the posterior.
        Its API is called Bayes' rule, which says
    </p>
    <p>
        $p(w | \mathcal{D}) = \dfrac{p(\mathcal{D}|w)p(w)}{p(\mathcal{D})} = \dfrac{p(\mathcal{D}|w)p(w)}{\int_{w'} p(\mathcal{D}|w')p(w') dw'}$
        .
    </p>
    <p>
        Its call signature requires two input distributions, the <i>prior</i> $p(w)$, and the <i>likelihood</i> $p(\mathcal{D} | w)$, and returns the posterior $p(w | \mathcal{D})$.
        While the posterior distribution is easily expressed, it is typically expensive to compute due to the pesky integral over all possible values of $w$.
        For most neural network we can't evaluate this integral analytically.
        We could instead approximate it numerically, but this will only be practical for small neural networks, since $w$ represents all the weights and biases, so it becomes very high dimensional for deep networks.
    </p>
    <p>
        So we will need to address this computational issue, which we do in the next section.
        In the meantime, let's consider another upside of using the posterior distribution over weights $p(w|\mathcal{D})$ instead of the estimate $w^*$ from standard neural network training.
        In particular, knowing the posterior allows us to do probabilistic prediction by expressing a distribution over predicted output $\hat y$ as a function of new input $x$,
    </p>
    <p>
    $p(\hat y(x)| \mathcal{D}) = \int_{w} p(\hat y(x)| w) p(w | \mathcal{D}) dw = \mathbb{E}_{w \sim p(w|\mathcal{D})}[p(\hat y(x)|w)]$, <!-- why not p(w| \mathcal{D}, x) here...-->
    </p>
    <p>
        which we call the <i>predictive distribution</i>.
        Notice that we can express this distribution as the expectation of the single network likelihood under the posterior $p(w|\mathcal{D})$.
        This yields a nice interpretation of the predictive distribution as an <i>infinite ensemble</i> of networks<dt-cite key="blundell2015weight"></dt-cite>, with each network's contribution to the overall prediction weighted by the posterior likelihood of its weights given the training data.
        Also, it implies that we can approximate this infinite ensemble using a finite number of Monte Carlo samples from the posterior.
        Approximating the predictive distribution with only a single Monte Carlo sample is equivalent to using a single network $\hat y_w(x)$ with weights $w$ chosen at random from $p(w|\mathcal{D})$; in a slight abuse of terminology, we can think of this process as <i>sampling networks</i> from the posterior.
    </p>
     <!-- Exact Inference figures -->
    <figure style="position:relative; width:984px; height:470px;">
        <div id="exact_train" class='bordered' style="position:relative; width:984px; height:300px;"></div>
        <button type="button" onclick="metropolis.start()">Sample</button>
        <figcaption id="exact_caption" style="position:relative; width: 420px; height: 100px; left: 540px; top: 10px;">
            Press the button to display the prediction function $\hat y_w(x)$ for successively "sampled" networks, whose weights are drawn from the true posterior: $w \sim p(w|\mathcal{D}_{\text{train}})$.
            Once sampling begins, the running-average of the Monte Carlo approximation to the predictive distribution X (using Y samples (HOW MANY?)) is displayed in red.
            Computing the posterior Z with exact inference is costly, but since we only need to sample from it we can use the Metropolis algorithm.
        </figcaption>

        <!-- <button type="button" onclick="exact_inference_view.reset()">Clear</button> -->
    </figure>
   </p>
    <p>
        The demo above illustrates shows the prediction function for a sequence of networks sampled in this way, as well as the approximate predictive distribution computed as the running average of sampled networks.
        Remember that computing the posterior using exact inference is costly even for modest-sized networks.
        On the other hand, producing <i>samples</i> from this unknown distribution is often feasible using algorithms described in the next section, and we can aggregate a finite number of these samples to obtain an approximate posterior.
   </p>
    <p>
    But before we move on, let's briefly consider the implications of Bayesian inference, which replaces a <i>point estimate</i> of the weights $w^*$ and its corresponding prediction function $\hat y_{w^*}(x)$ from standard training with <i>inferred distributions</i> $p(w|\mathcal{D})$ and $p(\hat y_{w}(x)|\mathcal{D})$.
    An immediate benefit of specifying distributions over the model parameters and predictions is that we can quantify our <i>uncertainty</i> about these things, e.g., by computing their variance.
    This is especially relevant when learning from small datasets; standard neural net training will overfit for the reasons discussed above, but Bayesian inference will find the best explanation for the model parameters given the available data, which typically have high uncertainty when data is scarce.
    In the limit of dataset size far exceeding the number of neurons, the inferred distributions sharpen and begin to resemble the solutions from the standard training; for modern networks we don't typically reach this limit in practice, so having a notion of model uncertainty is helpful.
   </p>

    <h3>Sampling-based inference</h3>
        <p style="color:red;">
            TODO: discuss sampling-based methods to approximate the posterior
        </p>


   <h3>Variational inference</h3>
    <p style="color:red;">
        TODO: contrast sampling-based inference with gradient based inference, especially with their ability to scale.
    </p>

    <p>
        Sampling-based inference is good enough for many applications, especially where the sample space is reasonable and achieving an exact solution is relatively more important than its computational cost.
        As network and dataset size grow, however, the cost of sampling also grows.
        In these cases we can instead solve a related but distinct inference problem that admits a learning algorithm very similar algorithm to what we used for standard training; we'll pose the problem now and discuss the learning algorithm in the next section.
        <p>
            <i>Variational methods</i> use a parameterized <i>approximate posterior</i> $q_\phi(w)$ as a stand-in for the true posterior $p(w|\mathcal{D})$.
            Since we want $q$ to approximate $p$, we can simply write inference as minimizing the difference between the two distributions:
        <p>
            $\underset{\phi}{\text{minimize}} \quad D_{KL}(q_\phi(w)||p(w|\mathcal{D})$,
        </p>

        <p>
            where $D_{KL}$ captures our notion of dissimilarity between two distributions.
            It is called the <i>Kullback-Liebler diverence</i> (or KL divergence), and is defined as
        </p>
        <p>
        $\quad D_{KL}(q_\phi(w)||p(w|\mathcal{D}) \coloneqq \int_{w} q_\phi(w) \log \frac{q_\phi(w)}{p(w|\mathcal{D})} = \mathbb{E}_{w \sim q_\theta(w)}[\log q_\phi(w) - \log p(w|\mathcal{D})]$
        .
        </p>

   <!-- KL divergence figure -->
    <figure style="position:relative; width:984px; height:400px;">
        <div id="curve" class='bordered' style="position:relative; float:left; width:834px; height:300px;"></div>
        <form style="position:relative; margin-left: 854px;" onclick="divergence_curve.update(getmean() - 6, sdScaler(getsd()))">
            <input type="radio" name="divergence_type" value="KL" checked> KL<br>
            <input type="radio" name="divergence_type" value="reverse KL"> reverse KL<br>
            <input type="radio" name="divergence_type" value="JS"> JS
        </form>
        <div id="slider3" style="position:absolute; width:300px; height: 50px; left:20px; top: 320px;">
            <text class="figtext" style="top: -5px; left: 20px; position: relative;">Mean μ = 0.0</text>
        </div>
        <div id="slider4" style="position:absolute; width: 300px; height: 50px; left: 280px; top: 320px;">
            <text class="figtext" style="top: -5px; left: 20px; position: relative;">Standard Deviation = 1.0</text>
        </div>
        <figcaption id="Gaussiancaption" style="position:absolute; width: 420px; height: 90px; left: 540px; top: 320px;">
            In variational inference we seek to approximate a complicated distribution $p$ (black) with a simpler parameterized distribution $q$ (red), in this case a Gaussian.
            The shaded contour represents $q \log \frac{q}{p}$ and the diameter of the circle and its color represents its integral, the KL-divergence $D_{KL}(q||p)$.
            How should we select parameter values of $q$ if we want to minimize $D_{KL}(q||p)$?
            What about if we want to minimize one of the alternative metrics, reverse KL or Jensen-Shannon distance?
        </figcaption>
    </figure>

        <p>
            Why should we be content with $D_{KL}$ as our notion of dissimilarity?
            We might notice, for example, by definition it is not symmetrical, $D_{KL}(q||p) \neq D_{KL}(p||q)$, which is why it's called a divergence instead of a distance.
            In short, we will show the KL-divergence is a sensible choice of objective because minimizing it equivalently improves a lower bound on the expected log likelihood, which is what we originally set out to maximize.
            As a bonus, KL-divergence has some nice mathematical properties that make it quite generally useful as well as an information theoretic interpretation as the cost in bits of mismatch between $q$ and $p$ <dt-cite key="shlens2014notes"></dt-cite>.
            The demo fixes a $p$ that we'd like to approximate, and shows how the parameters of a Gaussian $q$ (its mean and variance) affect $D_{KL}(q||p)$.
            You can also see how the parameters of $q$ affect two alternative measures of dissimilarity: the reverse KL-divergence $D_{KL}(p||q)$ and Jensen-Shannon distance $0.5D_{KL}(p||0.5p+0.5q) + 0.5D_{KL}(q||0.5p+0.5q)$
        </p>
    <p style="color:red;">
        TODO: derive the ELBO from the KL perspective
        The probability density function of $q_\phi(w)$, which depends on its parameters $\phi$ in addition to $w$, is a design choice that we will need to specify.
        $$q^*(\theta) = \texttt{argmax} E_{q(\theta)} [ p(x|\theta) ] - D_{KL} [q(\theta)||p(\theta)].$$
        The solution is a good one when the approximate posterior $q(\theta)$ is close to the true posterior $p(\theta|x)$.
        We will use gradient-based optimization to solve the $\texttt{argmax}$ optimization, the same algorithm we used before to train our neural network.
   </p>

    <p style="color:red;">
    TODO: We should say something about the role of the KL-divergence here and reference the figure.
    It would be nice to figure out how to put a legend in the KL interactive figure to clarify things.
    </p>


    <h2>Optimizing with gradients</h2>
        <p>
            Variational inference reframes the computation of an integral as the optimization of its lower bound, which notably implies we can use tools from the optimization literature to approximately solve our inference problem.
            This includes optimizing with minibatch (a.k.a. "stochastic") gradients, like we did with standard neural network training.
            Thus, with a slight tweak, we can adapt SGD to variational inference in an algorithm called <i> Stochastic Variational Inference</i>.
        </p>

    <p style="color:red;">
    TODO: The purpose of this section is to discuss SGD vs. SVI in more detail, and provide the algorithms side-by-side.
    We should also cite relevant papers, e.g.,
    </p>

   <figure style="position:relative; width:800px; height:200px;">
       <div class="container" style="width: 800px;">
           <div id="svi-algo-box-left" style="display: table-cell;"></div>
           <div id="svi-algo-box-right" style="display: table-cell;"></div>
       </div>
   </figure>

    <pre id="hello-world-code">
    \begin{algorithmic}
    \PROCEDURE{SGD}{$x, y, w$}
        \WHILE {training not converged}
            \STATE $g = \text{BACKPROP}(x, y, w)$
            \STATE $w = w + \alpha g$
        \ENDWHILE
        \ENDPROCEDURE
    \end{algorithmic}
    </pre>

    <pre id="hello-world-code2">
    \begin{algorithmic}
    \PROCEDURE{SVI}{$x, y, w$}
        \WHILE {training not converged}
            \STATE ${\color{blue} \epsilon \sim \mathcal{N}(0, I)}$
            \STATE $g = \text{BACKPROP}(x, y, {\color{blue} w+\epsilon\sigma})$
            \STATE $w = w + \alpha g$
            \STATE ${\color{blue}\sigma = \sigma + \alpha\epsilon g}$
        \ENDWHILE
        \ENDPROCEDURE
    \end{algorithmic}
    </pre>

    <p style="color:red;">
    TODO: segueway into the figure and algo box
    </p>

    <!-- Variational Inference Layer -->
    <figure style="position:relative; width:984px; height:1075px;">
        <figcaption id="Var_inf_caption">
            Variational Inference
        </figcaption>
        <figcaption id="var_caption">
            Now we fit a distribution of weights to the underlying latent distribution.
        </figcaption>
        <div id="var_full" class='bordered' style="position:relative; width:984px; height:300px;"></div>
        <button type="button" onclick="bnn.train()">Train</button>
        <button type="button" onclick="bnn.sample()">Sample</button>
        <button type="button" onclick="bnn.stop()">Pause</button>

        <figcaption id="exact_caption" style="position:relative; width: 470px; height: 10px; left:500px">
            What if you sampled from the learned distribution?
        </figcaption>
        <div class="container" style="margin-top: 20px;">
            <div id="var_progress" class='bordered' style="display: table-cell; width:364px; height:200px;"></div>
            <div id="var_graph" style="display: table-cell; width:600px; height:200px;"></div>
        </div>

        <div class='container'>
            <div style="display: table; height: 100%">
                <div class='table-cell-wrap'>
                    <figcaption id="var_loss_train_title" class='little_caption'>
                        Training Loss
                    </figcaption>
                    <div id="var_loss_train" class='contour'> </div>
                </div>
                <div class='table-cell-wrap'>
                    <figcaption id="var_loss_caption" style="position: relative; width: 150px; left: 75px; right: 75px; top: 100px;">
                        The loss surface for the Variational Net. The training loss is plotted on the left, the validation loss is plotted on the right.
                    </figcaption>
                </div>
                <div class='table-cell-wrap'>
                    <figcaption id="var_loss_valid_title" class='little_caption'>
                        Test Loss
                    </figcaption>
                    <div id="var_loss_valid" class='contour'> </div>
                </div>
            </div>
        </div>
    </figure>

    <!-- VARIANCE FIGURE
        <figure style="position:relative; width:984px; height:350px;">
        <figcaption id="disagree">
            A Change in Perspective
        </figcaption>
        <div id="uncertain_mlp" style="position:relative; float:left; width:485px; height:300px; bottom: 10px; border: 1px solid rgba(0, 0, 0, 0.05);"></div>
        <div id="uncertain_bnn" style="position:relative; margin-left: 499px; width:485px; height:300px; bottom: 10px; border: 1px solid rgba(0, 0, 0, 0.05);"></div>
        <button type="button" name="uncertain_train" onclick="uncertainty.train()">Train</button>
        <button type="button" name="uncertain_reset" onclick="uncertainty.reset()">Reset</button>
    </figure> -->

    <!--
        HERE ARE SOME IDEAS FOR EXTRA STUFF TO TALK ABOUT, SPACE PERMITTING
    <h3>Automatic Differentiation Variational Inference</h3>
    <p>
        The reparameterization trick
        <dt-cite key="kingma2013autoencoding"></dt-cite>
        <dt-cite key="rezende2014stochastic"></dt-cite>

        ADVI
        <dt-cite key="kucukelbir2016automatic"></dt-cite>

        Bayes by backprop
        <dt-cite key="blundell2015weight"></dt-cite>
    </p>

    <H3>Connection to Dropout</H3>
    <p>
        Dropout can be seen as an approximation to stochastic variational inference, where we add weights that can turn off each unit entirely, and integrate out these weights.
        <dt-cite key="kingma2015variational"></dt-cite>
        <dt-cite key="gal2016dropout"></dt-cite>
    </p>

    <H3>Connection to Ensembles</H3>
    <p>
        asdf
    </p>

    <H3>Connection to Gradient Noise</H3>
    <p>
        asdf
    </p>
    -->
    <p>
        <!- ELLIOT Q: CAN WE FORMAT THIS LIKE A \citet IN LATEX? ->
    </p>

    <script src="assets/lib/parameters.js"></script>
    <script src="assets/lib/plotter.js"></script>
    <script src="assets/lib/divergence.js"></script>
    <script src="assets/lib/contour_plot.js"></script>
    <script src="assets/lib/mlp.js"></script>
    <script src="assets/lib/bnn.js"></script>
    <script src="assets/lib/full_bnn.js"></script>
    <script src="assets/lib/exact_inference.js"></script>
    <script src="assets/lib/metropolis.js"></script>
    <script src="assets/lib/net_lib.js"></script>
    <script src="assets/lib/seedrandom.min.js"></script>
    <script src="assets/iterates.js"></script>
    <script>
        // NN on sine
        var mlp = mlp(d3.select("#nn_full"), d3.select("#nn_loss_train"), d3.select("#nn_loss_valid"), d3.select('#nn_graph'));
        mlp.plot();

        var metropolis = metropolis_view(d3.select("#exact_train"));

        //DIVERGENCES
        var divergence_curve = divergence(d3.select("#curve"), 0, 1);
        var sdScaler = d3.scaleLinear().domain([0, 4]).range([0.5, 2.5])

        var sliderc = sliderGen([230, 40])
            .ticks([0, 3, 6, 9, 12])
            .ticktitles(function(d, i) {
                return ["-6", "-3", "0", "3", "6"][i]
            })
            .change(function(i) {
                d3.select("#slider3").selectAll(".figtext").html("Mean μ = " + (getmean() - 6).toPrecision(2))
                divergence_curve.update(getmean() - 6, sdScaler(getsd()))
            })
            .startxval(6)
            .cRadius(7)
            .shifty(-12)
            .margins(20, 20)
        var sliderd = sliderGen([230, 40])
            .ticks([0, 1, 2, 3, 4])
            .ticktitles(function(d, i) {
                return ["0.5", "1", "1.5", "2", "2.5"][i]
            })
            .change(function(i) {
                d3.select("#slider4").selectAll(".figtext").html("Standard Deviation = " + sdScaler(getsd()).toPrecision(2))
                divergence_curve.update(getmean() - 6, sdScaler(getsd()))
            })
            .cRadius(7)
            .shifty(-12)
            .startxval(1)
            .margins(20, 20)

        var getmean = sliderc(d3.select("#slider3")).xval
        var getsd = sliderd(d3.select("#slider4")).xval
        divergence_curve.draw_line();

        // variational inference
        var bnn = bnn(d3.select("#var_full"), d3.select("#var_loss_train"), d3.select("#var_loss_valid"), d3.select("#var_progress"), d3.select("#var_graph"));

        // variational inference
        var full_bnn = full_bnn_view(d3.select("#full_bnn"), d3.select("#bnn_graph"));
        // full_bnn.train();

        // SGD algo box
       var code = document.getElementById("hello-world-code").textContent;
       var code2 = document.getElementById("hello-world-code2").textContent;
       var parent_left = document.getElementById("svi-algo-box-left");
       var parent_right = document.getElementById("svi-algo-box-right");
       var options = {
            lineNumber: true
        };
        pseudocode.render(code, parent_left, options);
        pseudocode.render(code2, parent_right, options);
    </script>
</dt-article>

<dt-appendix>
  <h3>Acknowledgments</h3>
  <p style="color:red;">
    TODO: Write down our acknowledgments.
  </p>
  <p>
    Many of our demos rely on Karpathy's ConvNetJS library <dt-cite key="karpathy2015convnetjs"></dt-cite>, including the example code provided therein.

  <h3>Author contribution</h3>

  <p style="color:red;">
    TODO: List the author contributions.
  </p>
</dt-appendix>

<script>
    renderMathInElement(
        document.body, {
            delimiters: [{
                    left: "$$",
                    right: "$$",
                    display: true
                },
                {
                    left: "$",
                    right: "$",
                    display: false
                },
                {
                    left: "\\\\",
                    right: "\\\\",
                    display: true
                },
                {
                    left: "\\",
                    right: "\\",
                    display: false
                },
            ]
        }
    );
</script>

<script type="text/bibliography">
@inproceedings{blundell2015weight, title={Weight Uncertainty in Neural Network}, author={Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan}, booktitle={International Conference on Machine Learning}, pages={1613--1622}, year={2015}, url={https://arxiv.org/abs/1505.05424}}
    @inproceedings{gal2016dropout, title={Dropout as a Bayesian approximation: Representing model uncertainty in deep learning}, author={Gal, Yarin and Ghahramani, Zoubin}, booktitle={international conference on machine learning}, pages={1050--1059}, year={2016}}
    @inproceedings{graves2011practical, title={Practical variational inference for neural networks}, author={Graves, Alex}, booktitle={Advances in Neural Information Processing Systems}, pages={2348--2356}, year={2011} }
    @article{hinton2012improving, title={Improving neural networks by preventing co-adaptation of feature detectors}, author={Hinton, Geoffrey E and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R}, journal={arXiv preprint arXiv:1207.0580}, year={2012}, url={https://arxiv.org/abs/1207.0580} }
    @article{hoffman2013stochastic, title={Stochastic variational inference}, author={Hoffman, Matthew D and Blei, David M and Wang, Chong and Paisley, John}, journal={The Journal of Machine Learning Research}, volume={14}, number={1}, pages={1303--1347}, year={2013} }
    @misc{karpathy2015convnetjs, title={ConvNetJS: Javascript library for deep learning}, author={Karpathy, A}, year={2015}, url={https://github.com/karpathy/convnetjs} }
    @article{kingma2013autoencoding, author = {Diederik P. Kingma and Max Welling}, title = {Auto-Encoding Variational Bayes}, journal = {International Conference on Learning Representations}, year = {2014} }
    @inproceedings{kingma2015adam, title={{Adam}: A Method for Stochastic Optimization}, year={2015}, author={Kingma, Diederik and Ba, Jimmy}, booktitle={International Conference on Learning Representations}, }
    @inproceedings{kingma2015variational, title={Variational dropout and the local reparameterization trick}, author={Kingma, Diederik P and Salimans, Tim and Welling, Max}, booktitle={Advances in Neural Information Processing Systems}, pages={2575--2583}, year={2015} }
    @article{kucukelbir2016automatic, title={Automatic Differentiation Variational Inference}, author={Kucukelbir, Alp and Tran, Dustin and Ranganath, Rajesh and Gelman, Andrew and Blei, David M}, journal={arXiv preprint arXiv:1603.00788}, year={2016} }
    @phdthesis{neal1995bayesian, title={Bayesian learning for neural networks}, author={Neal, Radford M}, year={1995}, school={University of Toronto} }
    @article{paisley2012variational, title={Variational Bayesian inference with stochastic search}, author={Paisley, John and Blei, David and Jordan, Michael}, journal={arXiv preprint arXiv:1206.6430}, year={2012} }
    @article{rall1981automatic, title={Automatic differentiation: Techniques and applications}, author={Rall, Louis B}, year={1981}, publisher={Springer} }
    @inproceedings{rezende2014stochastic, title={Stochastic Backpropagation and Approximate Inference in Deep Generative Models}, author={Rezende, Danilo J and Mohamed, Shakir and Wierstra, Daan}, booktitle={Proceedings of the 31st International Conference on Machine Learning}, pages={1278--1286}, year={2014} }
    @article{robbins1951stochastic, title={A stochastic approximation method}, author={Robbins, Herbert and Monro, Sutton}, journal={The annals of mathematical statistics}, pages={400--407}, year={1951} }
    @article{rumelhart1986learning, title={Learning representations by back-propagating errors}, author={Rumelhart, David E and Hinton, Geoffrey E}, journal={Nature}, volume={323}, pages={9}, year={1986} }
    @inproceedings{schulman2015gradient, title={Gradient estimation using stochastic computation graphs}, author={Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter}, booktitle={Advances in Neural Information Processing Systems}, pages={3528--3536}, year={2015} }
    @article{shlens2014notes, title={Notes on Kullback-Leibler divergence and likelihood}, author={Shlens, Jonathon}, journal={arXiv preprint arXiv:1404.2000}, year={2014}
}   @phdthesis{speelpenning1980compiling, title={Compiling Fast Partial Derivatives of Functions Given by Algorithms}, author={Speelpenning, Bert}, year={1980}, school={University of Illinois at Urbana-Champaign} }
</script>
